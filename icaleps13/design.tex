\section{DESIGN}

Introduction to the chapter

\subsection{White Rabbit}

White Rabbit (WR~\cite{wr}) is a project which aims at creating
an Ethernet-based network with low-latency, deterministic
packet delivery and network-wide, transparent, high-accuracy
timing distribution. The White Rabbit Network (WRN) is
based on existing standards: Ethernet (IEEE 802.3~\cite{internet}),
Synchronous Ethernet (SyncE~\cite{sync}) and PTP~\cite{ptp}. 

A WRN consists of White Rabbit Nodes (nodes) and White
Rabbit Switches (switches) interconnected by fiber optics links. 
A node is considered the source and destination of information sent 
over the WRN. The information distributed over a WRN includes:

\begin{itemize}
    \item Timing - frequency and International Atomic Time.
    \item Data - Ethernet traffic between nodes.
\end{itemize}

\textit{to be finished}

\subsection{Timing System}

\subsubsection{Data Master}

The Data Master for the FAIR accelerator deals with various different tasks.

The first step is to take machine commands from the LHC Software
Architecture (LSA) and convert these into sequence programs,
depicting beam production chains.

The corresponding actions to take are pre-programmed into the endpoints.
The timing master will run a multiple of these sequence programs
in parallel and each is generating event datagrams containing action IDs
and their execution times. These are scheduled and handed over to a dispatcher
to be sent over the timing network. The sequence programs will communicate to
resolve dependencies in beam production chains and are able to take external
signals and interlocks into account.

The Data Master possesses a high end CPU running an
OS for easy interfacing to the control system, compatibility to 
FAIRs standard libraries and raw processing power as well as a 
Field Programmable Gate Array (FPGA) for parallelism, deterministic 
behavior and ultra low IO latency.

The Data Master itself subdivided into three parts:

\textit{CPU - API Block} \\
Its CPU is fed by the LHC Software
Architecture (LSA) with machine parameters derived from
physical requirements for beam production chains. These
parameters are converted into sequence programs (Fig. 1),
compiled and uploaded to the FPGA of the Data master.

\textit{FPGA - SoftCPU Cluster} \\
These programs are run in
parallel on SoftCPU macros residing in the FPGA. They
deal with sending out machine events paired with an exe-
cution time to WR nodes, reacting to interlocks and mutual
synchronisation. At the moment, 32 of these Soft CPUs are
foreseen in the Data Master, able to carry out 32 tasks full
parallel with IO service times of less than 50ns.

\textit{FPGA - Event Concentrator} \\
An event concentrator macro in the data master will act as a bridge to the WR
network. Its primary functions are aggregation of events
into Ethernet Frames and to schedule transmission of these
event messages over the timing network so they arrive on
time at the respective nodes.

The first version of the timing master was a single handwritten sequence program
with variable parameters, able to control the \textit{Saclay Ion source?????}.
The second generation is using hardware cores for specific tasks.
Programmable hardware timer interrupts were added to the scheduler.
The dispatcher core and the etherbone protocol encoder are now also hardware
macros. It also uses three soft cpus and as many sequence programs while now
employing message signalled interrupts for internal communication.
This version will still use fixed sequence programs with parameters rather
than code auto-generated by the LSA system. It is currently under development and
shortly awaiting final testing. It is meant to control the CRYRING accelerator
as a testbed for the final FAIR facility. The third generation of the Data Master should scale up to 32 soft CPUs and
will use sequence programs automatically generated by the LSA core. After being
tested on the CRYRING, it will be used to control GSI's SIS18 synchrotron and the UNILAC linear
accelerator. If the design and implementation are proven adequate, the final version will
be used to control the future FAIR facility.

\begin{figure}[htb]
   \centering
   \includegraphics*[scale=0.4]{fig/fatima7.eps}
   \caption{Mathias...}
   \label{fatima}
\end{figure}


\subsubsection{Timing Master}

Cesar

\subsubsection{Management Master}

At startup, the WR networking devices need essential configuration parameters (e.g. ip
address). During operation, the network has to be constantly monitored in
order to prevent, identify and solve malfunctions or breakdowns of devices. 
These tasks are carried out by specialized software (e.g DHCP) installed in the
so-called WR Management Master (WR MM). 

In the current WRN the MM is also a gateway between the corporative, timing and management 
network. All the WR switches are connected either to management and timing
network, while the WR nodes are only to the timing network. 

Currently the MM is serving ip addresses to the management port of switches and WR
nodes using BOOTP~\cite{bootp}. Information about the status of the network (e.g.
link up/down, synchronization status etc...)is gathered in the MM using Distributed Information Managment
tool. Finally, the WR Switches can boot using a NFS server in the MM for testing
purposes. 

In the future the MM will offer advance management and monitoring capabilities
using the SNMP~\cite{snmp} and sFlow~\cite{sflow}. These tools will allow to the 
WR network manager to anticipate networks problems and maintain the reliability
and robustness required for the timing system.


\subsubsection{Timing Network}

Cesar

\subsubsection{Timing Receiver Nodes}


\textbf{Form Factors}

\textbf{SCU}
The biggest number of timing receivers will be build in the formfactor
of the SCU (Scalable Control Unit).
It is planned to run around 1200 units in FAIR.
The SCU is a combination of a carrier board with an Arria II FPGA and a
COMExpress board running
linux. The communication between FPGA and COMExpress board is done via
PCIe. The carrier board is
equipped with the circuitry for White Rabbit, a DDR3 memory for support
of a soft cpu
and a parallel flash for text memory. Two SFP slots are available, one
is used for White Rabbit, one for future
use. Also available are several IOs as LEMO, EIA232, USB and Ethernet.

The SCU controls up to 12 slave cards via the SCU bus. For a smaller
number of slaves a backplane with
only 5 slots exists. So two SCUs can be run in one 19 inch crate.
The main use cases in FAIR for the SCU is the control of the ACU
(Adaptive Control Unit) for ramped power suplies,
control of radio frequency devices with FIB (FPGA Interface Board) and
the control of kicker modules
via MIL extension board.

\textbf{PEXARRIA}
Wesley

\textbf{EXPLODER}
Wesley

\textbf{VME}
Cesar


\textbf{common design SoC}
Wesley


\textbf{eca, tlu}
Wesley


\textbf{etherbone slave}
Wesley


\textbf{uniform access from pcie,vme,usb,ethernet}
Wesley


