\section{DESIGN}

\textit{write intro}

\subsection{White Rabbit}

White Rabbit (WR~\cite{wr}) is a protocol developed to
synchronize nodes in a packet-based network with sub-ns accuracy. WR is based 
on existing standards: Ethernet (IEEE 802.3~\cite{internet}),
Synchronous Ethernet (SyncE~\cite{sync}) and PTP~\cite{ptp}. 
A WR network offers low-latency, deterministic packet delivery and network-wide, 
transparent, high-accuracy timing distribution. 

The information distributed over a WRN includes:
\begin{itemize}
    \item Timing - frequency and time.
    \item Data - Ethernet traffic between nodes.
\end{itemize}

A WRN consists of White Rabbit Nodes (nodes) and White
Rabbit Switches (switches) interconnected by fiber optics links. 

\begin{figure}[htb]
   \centering
   \includegraphics*[scale=0.5]{fig/icaleps_networ.eps}
   \caption{WR Timing Network}
   \label{network}
\end{figure}


\subsection{Timing System}

\subsubsection{Data Master}
\textit{ it has to be rewrited by Mathias}

The Data Master for the FAIR accelerator deals with various different tasks.
The first step is to take machine commands from the LHC Software
Architecture (LSA) and convert these into sequence programs,
depicting beam production chains.

The corresponding actions to take are pre-programmed into the endpoints.
The timing master will run a multiple of these sequence programs
in parallel and each is generating event datagrams containing action IDs
and their execution times. These are scheduled and handed over to a dispatcher
to be sent over the timing network. The sequence programs will communicate to
resolve dependencies in beam production chains and are able to take external
signals and interlocks into account.


The Data Master possesses a high end CPU running an
OS for easy interfacing to the control system, compatibility to 
FAIRs standard libraries and raw processing power as well as a 
Field Programmable Gate Array (FPGA) for parallelism, deterministic 
behavior and ultra low IO latency.

The Data Master itself subdivided into three parts:

\textit{CPU - API Block} \\
Its CPU is fed by the LHC Software
Architecture (LSA) with machine parameters derived from
physical requirements for beam production chains. These
parameters are converted into sequence programs,
compiled and uploaded to the FPGA of the Data master.

\textit{FPGA - SoftCPU Cluster} \\
These programs are run in
parallel on SoftCPU macros residing in the FPGA. They
deal with sending out machine events paired with an execution time to WR nodes, 
reacting to interlocks and mutual synchronisation. At the moment, 32 of these Soft CPUs are
foreseen in the Data Master, able to carry out 32 tasks full parallel with IO service times of less than 50ns.

\textit{FPGA - Event Concentrator} \\
An event concentrator macro in the data master will act as a bridge to the WR
network. Its primary functions are aggregation of events
into Ethernet Frames and to schedule transmission of these
event messages over the timing network so they arrive on
time at the respective nodes. 

The first version of the timing master was a single handwritten sequence program
with variable parameters, able to control the \textit{Saclay Ion source?????}.
The second generation is using hardware cores for specific tasks.
Programmable hardware timer interrupts were added to the scheduler.
The dispatcher core and the etherbone protocol encoder are now also hardware
macros. It also uses three soft cpus and as many sequence programs while now
employing message signalled interrupts for internal communication.
This version will still use fixed sequence programs with parameters rather
than code auto-generated by the LSA system. It is currently under development and
shortly awaiting final testing. It is meant to control the CRYRING accelerator
as a testbed for the final FAIR facility. The third generation of the Data Master should scale up to 32 soft CPUs and
will use sequence programs automatically generated by the LSA core. After being
tested on the CRYRING, it will be used to control GSI's SIS18 synchrotron and the UNILAC linear
accelerator. If the design and implementation are proven adequate, the final version will
be used to control the future FAIR facility.

\begin{figure}[htb]
   \centering
   \includegraphics*[scale=0.33]{fig/fatima7.eps}
   \caption{Mathias...}
   \label{fatima}
\end{figure}


\subsubsection{Timing Master}
Timing is distributed in the WRN from a switch called Timing Master (TM) to all
the other TRNs/WR switches in the network. All the devices in the WRN lock their frequency 
and adjust their local clocks to that of the TM. The TM is basically a WR Switch
configured as Master Clock in the PTP protocol. Uses 10 Mhz and PPS signal from
a GPS for synthonization and the UTC Time for the synchronization of the
network. NTP protocol can also be used for getting the time.
The Fig.~\ref{network} shows that the TM is on top of the network
connected to the first layer of WR switches. In a WR test network deployed in
GSI, a WR Switch v3 is already synchronizing prototype TRNs scattered in the
campus.  

\subsubsection{Management Master}

At startup, the WR networking devices need essential configuration parameters (e.g. ip
address). During operation, the network has to be constantly monitored in
order to prevent, identify and solve malfunctions or breakdowns of devices. 
These tasks are carried out by specialized software (e.g DHCP) installed in the
so-called WR Management Master (WR MM). 

In the current WRN the MM is also a gateway between the corporative, timing and management 
network. All the WR switches are connected either to management and timing
network, while the WR nodes are only to the timing network. 

Currently the MM is serving ip addresses to the management port of switches and WR
nodes using BOOTP~\cite{bootp}. Information about the status of the network (e.g.
link up/down, synchronization status etc...)is gathered in the MM using Distributed Information Managment
tool. Finally, the WR Switches can boot using a NFS server in the MM for testing
purposes. 

In the future the MM will offer advance management and monitoring capabilities
using the SNMP~\cite{snmp} and sFlow~\cite{sflow}. These tools will allow to the 
WR network manager to anticipate networks problems and maintain the reliability
and robustness required for the timing system.


\subsubsection{Timing Receiver Nodes}

FAIR requires multiple form factor variants of timing receiver nodes.
In order to reduce the maintenance effort,
all of our different FPGAs utilize a common system on chip (SoC)
design Fig.~\ref{soc}.
This design is centered around the Wishbone bus system,
which combines the standard timing receiver functionality with 
the form factor specific bus interfaces.
The standard functionality included in every timing receiver
consists of White Rabbit, an Event-Condition-Action (ECA) scheduler,
and a timestamp latch unit (TLU).
The form factor specific interfaces fall into three categories.
First, master interfaces for controlling the timing receiver,
such as PCIe, VME, USB, and Etherbone.
Second, slave interfaces for controlling off-chip resources,
such as DDR3, SRAM, flash, daughter boards, and displays.
Third, raw IO interfaces suitable for capturing signals to 
timestamps using the TLU or generating high precision timing
signals using the ECA.
These last interfaces are generally LEMO, LVDS, or HDMI connectors.

\begin{figure}[htb]
   \centering
   \includegraphics*[scale=0.5]{fig/icaleps_soc.eps}
   \caption{TRN Common System on Chip Design}
   \label{soc}
\end{figure}


The ECA unit is a gateware component for producing control
signals at preprogrammed times.
The idea is to split high-level command events,
which should be carried out by multiple devices at a given time,
from the actions an individual timing receiver must take.
To this end, 
after attaching a timing receiver to a controlled device,
one programs the ECA's condition table.
This rules in this table 
specify which command events from the data master
require action by the timing receiver.
Furthermore, the rules may include a time offset to compensate 
for local delays due to the attached table length or delays
inherent to the controlled device.

As a concrete example, 
one could program the ECA to respond to ramp events
by outputting new set values to the magnet's power supply.
When the data master broadcasts a new field strength to apply in 200us,
all the timing receivers process this request.
Those timing receivers which control magnets on the ring
recognize this event requires their action.
They calculate when they must power their magnets to achieve
the 200us target and schedule an action for that time.
When the time arrives, the ECA executes the required action,
accurate to 8ns.
In the future,
we intend to leverage Altera's PLL phase shifting technology 
to reach 100ps accuracy.

Similar to the ECA unit,
every timing receiver includes a TLU unit.
For each input connected to the TLU, 
there is a timestamp queue.
The rising or falling edge of a signal on the input
causes an absolute timestamp to be recorded in the 
respective queue.
Currently these timestamps are only accurate to 8ns,
though we intend to improve this when time permits.

Although different form factors provide different physical control interfaces, 
we have unified all of them to a single C library interface.
A user of this library does not need to care how the timing 
receiver is attached to his computer.
He must only specify an appropriate address;
for example,
dev/wbm0 for PCIe or VME,
dev/ttyUSB0 for USB,
udp/192.168.100.100 for Etherbone.
Using this library, all devices in the timing receiver
can be automatically discovered using the self-describing bus standard.
Thus, one does not need to know the particular SoC address layout
for a given timing receiver.
Instead, one simply locates the component to control,
complains if it is missing,
and then proceeds to access it via the C interface to Wishbone.
This means, for example, 
that we can program the flash of all our form factors using the same 
software tool regardless of the physical interface.

Although the master interfaces for each form factor differ,
they all include a network connection in order to run White Rabbit.
It is possible to control the Wishbone bus of a timing receiver over
the network using the Etherbone protocol.
As with all other master interfaces to the SoC,
access proceeds via the same library calls.
Etherbone is simply a serial version of the Wishbone bus protocol.
We use this same protocol in the USB master interface.
Due to this network connectivity,
it is theoretically possible (though perhaps unwise)
to broadcast a firmware update to all timing receivers simultaneously.

\textbf{SCU}
Most timing receivers will be built in the 
Scalable Control Unit (SCU) form factor.
It is planned to run around 1200 units in FAIR.
The SCU is a combination of a carrier board with an Arria II FPGA and a
COMExpress board running Linux.
The communication between FPGA and COMExpress board is done via PCIe.
The carrier board connects the COMExpress Atom processor with
an Ethernet port, two USB ports, and a serial console.
The onboard FPGA is connected to two SFP slots, two LEMOs, DDR3, 
parallel flash, an LCD, and a serial console.
An SCU controls up to 12 slave cards via the SCU bus
and can connect to an optional daughter board for additional IOs.
The main use cases in FAIR for the SCU is the control of the ACU
(Adaptive Control Unit) for ramped power suplies,
control of radio frequency devices with FIB (FPGA Interface Board) and
the control of kicker modules via MIL extension board.

\textbf{PEXARIA}
The pexaria5 is a 4-lane PCIe card intended to be used in a standard PC.
It is based on the newer Arria V FPGA platform.
It sports up to four SFP cages, a 26-pin trigger bus interface,
a USB port, and an internal LCD.
In the future it will also host a daughter board with external IO
ports.
It is foreseen to be used in data acquistion for experiments
and beam instrumentation.
The data master might be implemented using this form factor.

\textbf{EXPLODER}
The exploder is a portable timing receiver with many IO ports.
It is a two-planed, enclosed, hand-sized device 
hosting an Arria II FPGA and four SFPs.
The top plane contains the application-specific IOs,
such as LCD display, LEMOs, LVDS, NIM, trigger bus, knobs, etc.
The only additional master interface it provides is USB.
It is intended to be used in any situation where a hosted device,
such as the VME/PEXARIA/SCU would not be available.

\textbf{VETAR}
The vetar is a VMEbus IEEE-1014 card intended to be hosted in VME crates.
It also based on the Arria II FPGA and is endowed with one SFP cage, SRAM, LCD
display, EEPROM memory and USB interface. LVDS and lemo I/Os are available on the main 
board or on the extension board with additional HDMI connector.

\subsubsection{Timing Network}

The Timing Network interconnects the above described components of the Timing
System using WR switches. A stable and continuous synchronization of all the
TRNs with an appropriate accuracy and reliable and deterministic distribution 
of time events are the key requirements. 
Both timing and data resilience against network failures is achieved 
using redundant connections. Lower layer protocols
stablish a spanning tree topology setting ports connected to
cyclic paths to block/passive state. Upper-bound delivery latency of timing events 
from the DM to TRNs is guaranteed using a QoS tagging [] of the traffic and Cut-through [] switching.

